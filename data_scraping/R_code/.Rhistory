# Print results with best perplexity on validation sample (based on likelihood which is easier to read)
res[which.max(res$ll_validation),]
alpha = .1
n_topics = 5
lda_results <- FitLdaModel(dtm = dtm_train,
k = n_topics, # number of topics
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
alpha = alpha, beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
model <- lda_results
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts.
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
plot(model$prevalence, model$alpha, xlab = "% topic prevalence", ylab = "alpha")
maxProb <- apply(lda_results$theta, 1, max)
hist(maxProb, main="Maximum topic probability", xlab="Maximum topic probability")
# Get the top M terms of each topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
model$top_terms
SummarizeTopics(lda_results)
# Get top 10 words with probabilities for visualization
phi <- lda_results$phi
top_term_table <- NULL
n_topics = 5
for (j in 1:n_topics) {
words <- t(phi[j,]) %>% order(decreasing=TRUE) %>% head(10) %>% phi[j,.]
top_term_table <- rbind(top_term_table, data.frame(topic=j,probability=words , term = labels(words)) )
}
# Create a table of top 10 words grouped by topic
text_top_terms <- top_term_table %>%
group_by(topic) %>%
top_n(10, probability) %>%
ungroup() %>%
arrange(topic, -probability)
# Create plots of top 10 words for each topic
perplot <- 6
for (i in 1:ceiling(n_topics/perplot))
{
p <- text_top_terms %>%
filter(topic > (i-1)*perplot & topic<=(i*perplot)) %>% # filter out topic
mutate(term = reorder_within(term, probability, topic)) %>% # show highest probability up top
ggplot(aes(term, probability, fill = factor(topic))) + # bar plot layout
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()+
scale_x_reordered()
show(p)
}
df <- read.csv("data/fully_cleaned_reviews.csv")
dtm <- CreateDtm(doc_vec = df[, "Review"], # character vector of documents
doc_names = df[, "id"])
set.seed(4321)
n_topics=10; # Number of different topics
lda_results <- FitLdaModel(dtm = dtm,
k = n_topics, # number of topic
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
#alpha: prior on topic probabilities - beta: prior on word probabilities
alpha = 0.1,beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
alpha = .1
n_topics = 10
lda_results <- FitLdaModel(dtm = dtm_train,
k = n_topics, # number of topics
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
alpha = alpha, beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
model <- lda_results
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts.
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
plot(model$prevalence, model$alpha, xlab = "% topic prevalence", ylab = "alpha")
maxProb <- apply(lda_results$theta, 1, max)
hist(maxProb, main="Maximum topic probability", xlab="Maximum topic probability")
# Get the top M terms of each topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
model$top_terms
SummarizeTopics(lda_results)
# Get top 10 words with probabilities for visualization
phi <- lda_results$phi
top_term_table <- NULL
n_topics = 5
for (j in 1:n_topics) {
words <- t(phi[j,]) %>% order(decreasing=TRUE) %>% head(10) %>% phi[j,.]
top_term_table <- rbind(top_term_table, data.frame(topic=j,probability=words , term = labels(words)) )
}
# Create a table of top 10 words grouped by topic
text_top_terms <- top_term_table %>%
group_by(topic) %>%
top_n(10, probability) %>%
ungroup() %>%
arrange(topic, -probability)
# Create plots of top 10 words for each topic
perplot <- 6
for (i in 1:ceiling(n_topics/perplot))
{
p <- text_top_terms %>%
filter(topic > (i-1)*perplot & topic<=(i*perplot)) %>% # filter out topic
mutate(term = reorder_within(term, probability, topic)) %>% # show highest probability up top
ggplot(aes(term, probability, fill = factor(topic))) + # bar plot layout
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()+
scale_x_reordered()
show(p)
}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(quanteda)
library(tm)
library(textmineR)
library(dplyr)
library(SnowballC)
library(tidytext)
library(text2vec)
library(ggplot2)
df <- read.csv("data/fully_cleaned_reviews.csv")
dtm <- CreateDtm(doc_vec = df[, "Review"], # character vector of documents
doc_names = df[, "id"])
set.seed(4321)
n_topics=10; # Number of different topics
lda_results <- FitLdaModel(dtm = dtm,
k = n_topics, # number of topic
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
#alpha: prior on topic probabilities - beta: prior on word probabilities
alpha = 0.1,beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
n <- nrow(dtm)
train <- sample(1:n, round(n * 0.80))
dtm_train <- dtm[train,]
dtm_val <- dtm[-train,]
res <- NULL
TMlist <- list()  # list to collect results
n_topics <- 10
for (n_topics in seq(5, 25, 5)) # Do for number of topics 10, 20, 30, 40, 50
{
print(n_topics)
lda_results <- FitLdaModel(dtm = dtm_train,
k = n_topics, # number of topic
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
alpha = 0.1,beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
# calculates word co-occurrence for moving window (M=5) and contrasts with topic structure
coh_train <- mean(CalcProbCoherence(phi = lda_results$phi, dtm = dtm_train, M = 5) )
coh_val <- mean(CalcProbCoherence(phi = lda_results$phi, dtm = dtm_val, M = 5) )
# Calculate the log likelihood of the DTM for training set
ll_train <- CalcLikelihood(dtm = dtm_train,
phi = lda_results$phi,
theta = lda_results$theta)/nrow(dtm_train)
# Determine theta for validation set
lda_results$theta_val <- predict(lda_results, dtm_val, method = "gibbs", iterations = 700 + 10*n_topics, burnin = 200 + 10*n_topics)
# Calculate the log likelihood of the DTM for validation set
ll_val <- CalcLikelihood(dtm = dtm_val,
phi = lda_results$phi,
theta = lda_results$theta_val)/nrow(dtm_val)
# Combine all values in matrix per row
res <- rbind(res, data.frame(n_topics=n_topics, ll_train = ll_train, ll_validation = ll_val, coh_train=coh_train, coh_val=coh_val))
# Add current LDA result
TMlist <- append(TMlist, c(lda_results))
print(res)
}
# Make plot showing the coherence for different numbers of topics
ggplot(res, aes(n_topics)) +
geom_line(aes(y=coh_train, colour="Train")) + # show coherence training set
geom_line(aes(y=coh_val, colour="Validation")) # show coherence validation set
# Print results with best coherence
res[which.max(res$coh_val),]
ggplot(res, aes(n_topics)) +
geom_line(aes(y=ll_train, colour="Train")) + # show perplexity training set
geom_line(aes(y=ll_validation, colour="Validation")) + # show perplexity validation set
labs(colour="Sample")
# Print results with best perplexity on validation sample (based on likelihood which is easier to read)
res[which.max(res$ll_validation),]
alpha = .1
n_topics = 10
lda_results <- FitLdaModel(dtm = dtm_train,
k = n_topics, # number of topics
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
alpha = alpha, beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
model <- lda_results
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts.
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
plot(model$prevalence, model$alpha, xlab = "% topic prevalence", ylab = "alpha")
maxProb <- apply(lda_results$theta, 1, max)
hist(maxProb, main="Maximum topic probability", xlab="Maximum topic probability")
# Get the top M terms of each topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
model$top_terms
SummarizeTopics(lda_results)
# Get top 10 words with probabilities for visualization
phi <- lda_results$phi
top_term_table <- NULL
n_topics = 15
for (j in 1:n_topics) {
words <- t(phi[j,]) %>% order(decreasing=TRUE) %>% head(10) %>% phi[j,.]
top_term_table <- rbind(top_term_table, data.frame(topic=j,probability=words , term = labels(words)) )
}
alpha = .1
n_topics = 15
lda_results <- FitLdaModel(dtm = dtm_train,
k = n_topics, # number of topics
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
alpha = alpha, beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
model <- lda_results
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts.
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
plot(model$prevalence, model$alpha, xlab = "% topic prevalence", ylab = "alpha")
# Get the top M terms of each topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
model$top_terms
SummarizeTopics(lda_results)
# Get top 10 words with probabilities for visualization
phi <- lda_results$phi
top_term_table <- NULL
n_topics = 15
for (j in 1:n_topics) {
words <- t(phi[j,]) %>% order(decreasing=TRUE) %>% head(10) %>% phi[j,.]
top_term_table <- rbind(top_term_table, data.frame(topic=j,probability=words , term = labels(words)) )
}
# Create a table of top 10 words grouped by topic
text_top_terms <- top_term_table %>%
group_by(topic) %>%
top_n(10, probability) %>%
ungroup() %>%
arrange(topic, -probability)
# Create plots of top 10 words for each topic
perplot <- 6
for (i in 1:ceiling(n_topics/perplot))
{
p <- text_top_terms %>%
filter(topic > (i-1)*perplot & topic<=(i*perplot)) %>% # filter out topic
mutate(term = reorder_within(term, probability, topic)) %>% # show highest probability up top
ggplot(aes(term, probability, fill = factor(topic))) + # bar plot layout
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()+
scale_x_reordered()
show(p)
}
# Get top 10 words with probabilities for visualization
phi <- lda_results$phi
top_term_table <- NULL
n_topics = 15
for (j in 1:n_topics) {
words <- t(phi[j,]) %>% order(decreasing=TRUE) %>% head(10) %>% phi[j,.]
top_term_table <- rbind(top_term_table, data.frame(topic=j,probability=words , term = labels(words)) )
}
# Create a table of top 10 words grouped by topic
text_top_terms <- top_term_table %>%
group_by(topic) %>%
top_n(10, probability) %>%
ungroup() %>%
arrange(topic, -probability)
# Create plots of top 10 words for each topic
perplot <- 6
for (i in 1:ceiling(n_topics/perplot))
{
p <- text_top_terms %>%
filter(topic > (i-1)*perplot & topic<=(i*perplot)) %>% # filter out topic
mutate(term = reorder_within(term, probability, topic)) %>% # show highest probability up top
ggplot(aes(term, probability, fill = factor(topic))) + # bar plot layout
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()+
scale_x_reordered()
show(p)
}
library(rvest)
page <- read_html(url)
url <- "https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+9+7950X&id=5031"
page <- read_html(url)
DDR5_names <- page %>%
html_elements("//*[@id="placeholder"]/div/div[2]/div") %>%
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
DDR5_names
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
View(df)
View(release_dates)
url <- "https://www.cpubenchmark.net/singleThread.html"
page <- read_html(url)
names <- page %>%
html_nodes("#desktop-thread .prdname") %>%
html_text()
names <- as.matrix(names)
cpulink <- page %>%
html_nodes("#desktop-thread a") %>%
html_attr("href") %>% paste0("https://www.cpubenchmark.net/",.)
cpulink <- as.matrix(cpulink)
# Convert the vector into a matrix with 3 columns
matrix_form_scores <- matrix(names, byrow = TRUE)
# Convert the matrix into a dataframe
CPU_scores_df <- as.data.frame(matrix_form_scores)
# Optionally, set column names
colnames(CPU_scores_df) <- c("CPU")
CPU_scores_df$URL <- unlist(cpulink)
release_dates <- vector("list", length = nrow(CPU_scores_df))
current_prices <- vector("list", length = nrow(CPU_scores_df))
cpu_scores <- vector("list", length = nrow(CPU_scores_df))
for (i in 1:nrow(CPU_scores_df)) {
# Construct the URL for the CPU's page
cpu_url <- CPU_scores_df$URL[i]
# Scrape the CPU's page
cpu_page <- read_html(cpu_url)
# Extract the specific information you're interested in
# Note: Replace the '.selector' with the actual CSS selector of the data you want
cpu_detail <- cpu_page %>%
html_nodes('.alt:nth-child(2) , p:nth-child(5) , .speedicon+ span') %>%
html_text()
# Store the extracted detail in the list
release_date_temp <- str_trim(unlist(strsplit(cpu_detail[1], ":"))[2])
current_price_temp <- str_trim(unlist(strsplit(cpu_detail[2], ":"))[2])
score_temp <- cpu_detail[3]
release_dates[[i]] <- release_date_temp
current_prices[[i]] <- current_price_temp
cpu_scores <- score_temp
# Optional: Print progress
cat("Scraped data for", CPU_scores_df$CPU[i], "\n")
}
CPU_scores_df$ReleaseDate <- unlist(release_dates)
CPU_scores_df$CurrentPrice2 <- unlist(current_prices)
if (!require("rvest")) install.packages("rvest")
if (!require("stringr")) install.packages("stringr")
if (!require("dplyr")) install.packages("dplyr")
if (!require("tidyr")) install.packages("tidyr")
computeData <- TRUE
url <- "https://www.cpubenchmark.net/singleThread.html"
page <- read_html(url)
names <- page %>%
html_nodes("#desktop-thread .prdname") %>%
html_text()
names <- as.matrix(names)
cpulink <- page %>%
html_nodes("#desktop-thread a") %>%
html_attr("href") %>% paste0("https://www.cpubenchmark.net/",.)
cpulink <- as.matrix(cpulink)
# Convert the vector into a matrix with 3 columns
matrix_form_scores <- matrix(names, byrow = TRUE)
# Convert the matrix into a dataframe
CPU_scores_df <- as.data.frame(matrix_form_scores)
# Optionally, set column names
colnames(CPU_scores_df) <- c("CPU")
CPU_scores_df$URL <- unlist(cpulink)
release_dates <- vector("list", length = nrow(CPU_scores_df))
current_prices <- vector("list", length = nrow(CPU_scores_df))
cpu_scores <- vector("list", length = nrow(CPU_scores_df))
# Construct the URL for the CPU's page
cpu_url <- CPU_scores_df$URL[1]
# Scrape the CPU's page
cpu_page <- read_html(cpu_url)
# Extract the specific information you're interested in
# Note: Replace the '.selector' with the actual CSS selector of the data you want
cpu_detail <- cpu_page %>%
html_nodes('.alt:nth-child(2) , p:nth-child(5) , .speedicon+ span') %>%
html_text()
# Store the extracted detail in the list
release_date_temp <- str_trim(unlist(strsplit(cpu_detail[1], ":"))[2])
current_price_temp <- str_trim(unlist(strsplit(cpu_detail[2], ":"))[2])
score_temp <- cpu_detail[3]
url <- "https://www.cpubenchmark.net/singleThread.html"
page <- read_html(url)
names <- page %>%
html_nodes("#desktop-thread .prdname") %>%
html_text()
names <- as.matrix(names)
cpulink <- page %>%
html_nodes("#desktop-thread a") %>%
html_attr("href") %>% paste0("https://www.cpubenchmark.net/",.)
cpulink <- as.matrix(cpulink)
# Convert the vector into a matrix with 3 columns
matrix_form_scores <- matrix(names, byrow = TRUE)
# Convert the matrix into a dataframe
CPU_scores_df <- as.data.frame(matrix_form_scores)
# Optionally, set column names
colnames(CPU_scores_df) <- c("CPU")
CPU_scores_df$URL <- unlist(cpulink)
release_dates <- vector("list", length = nrow(CPU_scores_df))
current_prices <- vector("list", length = nrow(CPU_scores_df))
cpu_scores <- vector("list", length = nrow(CPU_scores_df))
# Construct the URL for the CPU's page
cpu_url <- CPU_scores_df$URL[1]
# Scrape the CPU's page
cpu_page <- read_html(cpu_url)
# Extract the specific information you're interested in
# Note: Replace the '.selector' with the actual CSS selector of the data you want
cpu_detail <- cpu_page %>%
html_nodes('.alt:nth-child(2) , p:nth-child(5) , .speedicon+ span') %>%
html_text()
# Store the extracted detail in the list
release_date_temp <- str_trim(unlist(strsplit(cpu_detail[1], ":"))[2])
current_price_temp <- str_trim(unlist(strsplit(cpu_detail[2], ":"))[2])
score_temp <- cpu_detail[3]
# Construct the URL for the CPU's page
cpu_url <- CPU_scores_df$URL[100]
# Scrape the CPU's page
cpu_page <- read_html(cpu_url)
# Extract the specific information you're interested in
# Note: Replace the '.selector' with the actual CSS selector of the data you want
cpu_detail <- cpu_page %>%
html_nodes('.alt:nth-child(2) , p:nth-child(5) , .speedicon+ span') %>%
html_text()
# Store the extracted detail in the list
release_date_temp <- str_trim(unlist(strsplit(cpu_detail[1], ":"))[2])
current_price_temp <- str_trim(unlist(strsplit(cpu_detail[2], ":"))[2])
score_temp <- cpu_detail[3]
release_dates[[i]] <- release_date_temp
current_prices[[i]] <- current_price_temp
cpu_url
# Construct the URL for the CPU's page
cpu_url <- CPU_scores_df$URL[100]
# Scrape the CPU's page
cpu_page <- read_html(cpu_url)
# Extract the specific information you're interested in
# Note: Replace the '.selector' with the actual CSS selector of the data you want
cpu_detail <- cpu_page %>%
html_nodes('.desc-foot p:nth-child(5) , .speedicon+ span') %>%
html_text()
# Store the extracted detail in the list
release_date_temp <- str_trim(unlist(strsplit(cpu_detail[1], ":"))[2])
score_temp <- cpu_detail[2]
# Construct the URL for the CPU's page
cpu_url <- CPU_scores_df$URL[1]
# Scrape the CPU's page
cpu_page <- read_html(cpu_url)
# Extract the specific information you're interested in
# Note: Replace the '.selector' with the actual CSS selector of the data you want
cpu_detail <- cpu_page %>%
html_nodes('.desc-foot p:nth-child(5) , .speedicon+ span') %>%
html_text()
# Store the extracted detail in the list
current_price_temp <- str_trim(unlist(strsplit(cpu_detail[1], ":"))[2])
score_temp <- cpu_detail[2]
# Construct the URL for the CPU's page
cpu_url <- CPU_scores_df$URL[1]
# Scrape the CPU's page
cpu_page <- read_html(cpu_url)
# Extract the specific information you're interested in
# Note: Replace the '.selector' with the actual CSS selector of the data you want
cpu_detail <- cpu_page %>%
html_nodes('.alt:nth-child(2) , .desc-foot p:nth-child(5) , .speedicon+ span') %>%
html_text()
# Store the extracted detail in the list
current_price_temp <- str_trim(unlist(strsplit(cpu_detail[1], ":"))[2])
# Construct the URL for the CPU's page
cpu_url <- CPU_scores_df$URL[100]
# Scrape the CPU's page
cpu_page <- read_html(cpu_url)
# Extract the specific information you're interested in
# Note: Replace the '.selector' with the actual CSS selector of the data you want
cpu_detail <- cpu_page %>%
html_nodes('.alt:nth-child(2) , .desc-foot p:nth-child(5) , .speedicon+ span') %>%
html_text()
# Store the extracted detail in the list
release_date_temp <- str_trim(unlist(strsplit(cpu_detail[1], ":"))[2])
current_price_temp <- str_trim(unlist(strsplit(cpu_detail[2], ":"))[2])
score_temp <- cpu_detail[3]
# Construct the URL for the CPU's page
cpu_url <- CPU_scores_df$URL[200]
# Scrape the CPU's page
cpu_page <- read_html(cpu_url)
# Extract the specific information you're interested in
# Note: Replace the '.selector' with the actual CSS selector of the data you want
cpu_detail <- cpu_page %>%
html_nodes('.alt:nth-child(2) , .desc-foot p:nth-child(5) , .speedicon+ span') %>%
html_text()
# Store the extracted detail in the list
release_date_temp <- str_trim(unlist(strsplit(cpu_detail[1], ":"))[2])
current_price_temp <- str_trim(unlist(strsplit(cpu_detail[2], ":"))[2])
score_temp <- cpu_detail[3]
setwd("D:/Erasmus University/thesis_PC_LTE/data_scraping/R_code")
if (!require("rvest")) install.packages("rvest")
if (!require("stringr")) install.packages("stringr")
if (!require("dplyr")) install.packages("dplyr")
if (!require("tidyr")) install.packages("tidyr")
source("D:/Erasmus University/thesis_PC_LTE/data_scraping/R_code/CPU_Data.R", echo=TRUE)
View(CPU_scores_df)
source("D:/Erasmus University/thesis_PC_LTE/data_scraping/R_code/CPU_Data.R", echo=TRUE)
View(CPU_df)
setwd("D:/Erasmus University/thesis_PC_LTE/data_scraping/R_code")
write.csv(CPU_df, file = "Component_data/cpu_data_r.csv", row.names = FALSE)
write.csv(CPU_df, file = "Component_data/cpu_data_r.csv", row.names = FALSE)
write.csv(CPU_df, file = "../scraped_data/cpu_data_r.csv", row.names = FALSE)
View(CPU_df)
