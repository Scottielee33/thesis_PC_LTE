library(text2vec)
df <- read.csv("data/fully_cleaned_reviews.csv")
dtm <- CreateDtm(doc_vec = df[, "Review"], # character vector of documents
doc_names = df[, "id"])
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(quanteda)
library(tm)
library(tidytext)
library(dplyr)
library(SnowballC)
library(tidytext)
library(text2vec)
df <- read.csv("data/fully_cleaned_reviews.csv")
dtm <- CreateDtm(doc_vec = df[, "Review"], # character vector of documents
doc_names = df[, "id"])
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(quanteda)
library(tm)
library(topicmodels)
install.packages(" textmineR")
install.packages("textmineR")
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(quanteda)
library(tm)
library(textmineR)
library(dplyr)
library(SnowballC)
library(tidytext)
library(text2vec)
df <- read.csv("data/fully_cleaned_reviews.csv")
dtm <- CreateDtm(doc_vec = df[, "Review"], # character vector of documents
doc_names = df[, "id"])
set.seed(4321)
n_topics=10; # Number of different topics
lda_results <- FitLdaModel(dtm = dtm,
k = n_topics, # number of topic
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
#alpha: prior on topic probabilities - beta: prior on word probabilities
alpha = 0.1,beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(quanteda)
library(tm)
library(textmineR)
library(dplyr)
library(SnowballC)
library(tidytext)
library(text2vec)
df <- read.csv("data/fully_cleaned_reviews.csv")
dtm <- CreateDtm(doc_vec = df[, "Review"], # character vector of documents
doc_names = df[, "id"])
set.seed(4321)
n_topics=10; # Number of different topics
lda_results <- FitLdaModel(dtm = dtm,
k = n_topics, # number of topic
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
#alpha: prior on topic probabilities - beta: prior on word probabilities
alpha = 0.1,beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
n <- nrow(dtm)
train <- sample(1:n, round(n * 0.80))
dtm_train <- dtm[train,]
dtm_val <- dtm[-train,]
res <- NULL
TMlist <- list()  # list to collect results
n_topics <- 10
for (n_topics in seq(5, 25, 5)) # Do for number of topics 10, 20, 30, 40, 50
{
print(n_topics)
lda_results <- FitLdaModel(dtm = dtm_train,
k = n_topics, # number of topic
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
alpha = 0.1,beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
# calculates word co-occurrence for moving window (M=5) and contrasts with topic structure
coh_train <- mean(CalcProbCoherence(phi = lda_results$phi, dtm = dtm_train, M = 5) )
coh_val <- mean(CalcProbCoherence(phi = lda_results$phi, dtm = dtm_val, M = 5) )
# Calculate the log likelihood of the DTM for training set
ll_train <- CalcLikelihood(dtm = dtm_train,
phi = lda_results$phi,
theta = lda_results$theta)/nrow(dtm_train)
# Determine theta for validation set
lda_results$theta_val <- predict(lda_results, dtm_val, method = "gibbs", iterations = 700 + 10*n_topics, burnin = 200 + 10*n_topics)
# Calculate the log likelihood of the DTM for validation set
ll_val <- CalcLikelihood(dtm = dtm_val,
phi = lda_results$phi,
theta = lda_results$theta_val)/nrow(dtm_val)
# Combine all values in matrix per row
res <- rbind(res, data.frame(n_topics=n_topics, ll_train = ll_train, ll_validation = ll_val, coh_train=coh_train, coh_val=coh_val))
# Add current LDA result
TMlist <- append(TMlist, c(lda_results))
print(res)
}
# Make plot showing the coherence for different numbers of topics
ggplot(res, aes(n_topics)) +
geom_line(aes(y=coh_train, colour="Train")) + # show coherence training set
geom_line(aes(y=coh_val, colour="Validation")) # show coherence validation set
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(quanteda)
library(tm)
library(textmineR)
library(dplyr)
library(SnowballC)
library(tidytext)
library(text2vec)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(quanteda)
library(tm)
library(textmineR)
library(dplyr)
library(SnowballC)
library(tidytext)
library(text2vec)
library(ggplot2)
df <- read.csv("data/fully_cleaned_reviews.csv")
dtm <- CreateDtm(doc_vec = df[, "Review"], # character vector of documents
doc_names = df[, "id"])
set.seed(4321)
n_topics=10; # Number of different topics
lda_results <- FitLdaModel(dtm = dtm,
k = n_topics, # number of topic
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
#alpha: prior on topic probabilities - beta: prior on word probabilities
alpha = 0.1,beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
n <- nrow(dtm)
train <- sample(1:n, round(n * 0.80))
dtm_train <- dtm[train,]
dtm_val <- dtm[-train,]
res <- NULL
TMlist <- list()  # list to collect results
n_topics <- 10
for (n_topics in seq(5, 25, 5)) # Do for number of topics 10, 20, 30, 40, 50
{
print(n_topics)
lda_results <- FitLdaModel(dtm = dtm_train,
k = n_topics, # number of topic
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
alpha = 0.1,beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
# calculates word co-occurrence for moving window (M=5) and contrasts with topic structure
coh_train <- mean(CalcProbCoherence(phi = lda_results$phi, dtm = dtm_train, M = 5) )
coh_val <- mean(CalcProbCoherence(phi = lda_results$phi, dtm = dtm_val, M = 5) )
# Calculate the log likelihood of the DTM for training set
ll_train <- CalcLikelihood(dtm = dtm_train,
phi = lda_results$phi,
theta = lda_results$theta)/nrow(dtm_train)
# Determine theta for validation set
lda_results$theta_val <- predict(lda_results, dtm_val, method = "gibbs", iterations = 700 + 10*n_topics, burnin = 200 + 10*n_topics)
# Calculate the log likelihood of the DTM for validation set
ll_val <- CalcLikelihood(dtm = dtm_val,
phi = lda_results$phi,
theta = lda_results$theta_val)/nrow(dtm_val)
# Combine all values in matrix per row
res <- rbind(res, data.frame(n_topics=n_topics, ll_train = ll_train, ll_validation = ll_val, coh_train=coh_train, coh_val=coh_val))
# Add current LDA result
TMlist <- append(TMlist, c(lda_results))
print(res)
}
# Make plot showing the coherence for different numbers of topics
ggplot(res, aes(n_topics)) +
geom_line(aes(y=coh_train, colour="Train")) + # show coherence training set
geom_line(aes(y=coh_val, colour="Validation")) # show coherence validation set
# Print results with best coherence
res[which.max(res$coh_val),]
ggplot(res, aes(n_topics)) +
geom_line(aes(y=ll_train, colour="Train")) + # show perplexity training set
geom_line(aes(y=ll_validation, colour="Validation")) + # show perplexity validation set
labs(colour="Sample")
# Print results with best perplexity on validation sample (based on likelihood which is easier to read)
res[which.max(res$ll_validation),]
alpha = .1
n_topics = 5
lda_results <- FitLdaModel(dtm = dtm_train,
k = n_topics, # number of topics
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
alpha = alpha, beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
model <- lda_results
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts.
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
plot(model$prevalence, model$alpha, xlab = "% topic prevalence", ylab = "alpha")
maxProb <- apply(lda_results$theta, 1, max)
hist(maxProb, main="Maximum topic probability", xlab="Maximum topic probability")
# Get the top M terms of each topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
model$top_terms
SummarizeTopics(lda_results)
# Get top 10 words with probabilities for visualization
phi <- lda_results$phi
top_term_table <- NULL
n_topics = 5
for (j in 1:n_topics) {
words <- t(phi[j,]) %>% order(decreasing=TRUE) %>% head(10) %>% phi[j,.]
top_term_table <- rbind(top_term_table, data.frame(topic=j,probability=words , term = labels(words)) )
}
# Create a table of top 10 words grouped by topic
text_top_terms <- top_term_table %>%
group_by(topic) %>%
top_n(10, probability) %>%
ungroup() %>%
arrange(topic, -probability)
# Create plots of top 10 words for each topic
perplot <- 6
for (i in 1:ceiling(n_topics/perplot))
{
p <- text_top_terms %>%
filter(topic > (i-1)*perplot & topic<=(i*perplot)) %>% # filter out topic
mutate(term = reorder_within(term, probability, topic)) %>% # show highest probability up top
ggplot(aes(term, probability, fill = factor(topic))) + # bar plot layout
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()+
scale_x_reordered()
show(p)
}
df <- read.csv("data/fully_cleaned_reviews.csv")
dtm <- CreateDtm(doc_vec = df[, "Review"], # character vector of documents
doc_names = df[, "id"])
set.seed(4321)
n_topics=10; # Number of different topics
lda_results <- FitLdaModel(dtm = dtm,
k = n_topics, # number of topic
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
#alpha: prior on topic probabilities - beta: prior on word probabilities
alpha = 0.1,beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
alpha = .1
n_topics = 10
lda_results <- FitLdaModel(dtm = dtm_train,
k = n_topics, # number of topics
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
alpha = alpha, beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
model <- lda_results
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts.
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
plot(model$prevalence, model$alpha, xlab = "% topic prevalence", ylab = "alpha")
maxProb <- apply(lda_results$theta, 1, max)
hist(maxProb, main="Maximum topic probability", xlab="Maximum topic probability")
# Get the top M terms of each topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
model$top_terms
SummarizeTopics(lda_results)
# Get top 10 words with probabilities for visualization
phi <- lda_results$phi
top_term_table <- NULL
n_topics = 5
for (j in 1:n_topics) {
words <- t(phi[j,]) %>% order(decreasing=TRUE) %>% head(10) %>% phi[j,.]
top_term_table <- rbind(top_term_table, data.frame(topic=j,probability=words , term = labels(words)) )
}
# Create a table of top 10 words grouped by topic
text_top_terms <- top_term_table %>%
group_by(topic) %>%
top_n(10, probability) %>%
ungroup() %>%
arrange(topic, -probability)
# Create plots of top 10 words for each topic
perplot <- 6
for (i in 1:ceiling(n_topics/perplot))
{
p <- text_top_terms %>%
filter(topic > (i-1)*perplot & topic<=(i*perplot)) %>% # filter out topic
mutate(term = reorder_within(term, probability, topic)) %>% # show highest probability up top
ggplot(aes(term, probability, fill = factor(topic))) + # bar plot layout
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()+
scale_x_reordered()
show(p)
}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(quanteda)
library(tm)
library(textmineR)
library(dplyr)
library(SnowballC)
library(tidytext)
library(text2vec)
library(ggplot2)
df <- read.csv("data/fully_cleaned_reviews.csv")
dtm <- CreateDtm(doc_vec = df[, "Review"], # character vector of documents
doc_names = df[, "id"])
set.seed(4321)
n_topics=10; # Number of different topics
lda_results <- FitLdaModel(dtm = dtm,
k = n_topics, # number of topic
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
#alpha: prior on topic probabilities - beta: prior on word probabilities
alpha = 0.1,beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
n <- nrow(dtm)
train <- sample(1:n, round(n * 0.80))
dtm_train <- dtm[train,]
dtm_val <- dtm[-train,]
res <- NULL
TMlist <- list()  # list to collect results
n_topics <- 10
for (n_topics in seq(5, 25, 5)) # Do for number of topics 10, 20, 30, 40, 50
{
print(n_topics)
lda_results <- FitLdaModel(dtm = dtm_train,
k = n_topics, # number of topic
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
alpha = 0.1,beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
# calculates word co-occurrence for moving window (M=5) and contrasts with topic structure
coh_train <- mean(CalcProbCoherence(phi = lda_results$phi, dtm = dtm_train, M = 5) )
coh_val <- mean(CalcProbCoherence(phi = lda_results$phi, dtm = dtm_val, M = 5) )
# Calculate the log likelihood of the DTM for training set
ll_train <- CalcLikelihood(dtm = dtm_train,
phi = lda_results$phi,
theta = lda_results$theta)/nrow(dtm_train)
# Determine theta for validation set
lda_results$theta_val <- predict(lda_results, dtm_val, method = "gibbs", iterations = 700 + 10*n_topics, burnin = 200 + 10*n_topics)
# Calculate the log likelihood of the DTM for validation set
ll_val <- CalcLikelihood(dtm = dtm_val,
phi = lda_results$phi,
theta = lda_results$theta_val)/nrow(dtm_val)
# Combine all values in matrix per row
res <- rbind(res, data.frame(n_topics=n_topics, ll_train = ll_train, ll_validation = ll_val, coh_train=coh_train, coh_val=coh_val))
# Add current LDA result
TMlist <- append(TMlist, c(lda_results))
print(res)
}
# Make plot showing the coherence for different numbers of topics
ggplot(res, aes(n_topics)) +
geom_line(aes(y=coh_train, colour="Train")) + # show coherence training set
geom_line(aes(y=coh_val, colour="Validation")) # show coherence validation set
# Print results with best coherence
res[which.max(res$coh_val),]
ggplot(res, aes(n_topics)) +
geom_line(aes(y=ll_train, colour="Train")) + # show perplexity training set
geom_line(aes(y=ll_validation, colour="Validation")) + # show perplexity validation set
labs(colour="Sample")
# Print results with best perplexity on validation sample (based on likelihood which is easier to read)
res[which.max(res$ll_validation),]
alpha = .1
n_topics = 10
lda_results <- FitLdaModel(dtm = dtm_train,
k = n_topics, # number of topics
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
alpha = alpha, beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
model <- lda_results
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts.
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
plot(model$prevalence, model$alpha, xlab = "% topic prevalence", ylab = "alpha")
maxProb <- apply(lda_results$theta, 1, max)
hist(maxProb, main="Maximum topic probability", xlab="Maximum topic probability")
# Get the top M terms of each topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
model$top_terms
SummarizeTopics(lda_results)
# Get top 10 words with probabilities for visualization
phi <- lda_results$phi
top_term_table <- NULL
n_topics = 15
for (j in 1:n_topics) {
words <- t(phi[j,]) %>% order(decreasing=TRUE) %>% head(10) %>% phi[j,.]
top_term_table <- rbind(top_term_table, data.frame(topic=j,probability=words , term = labels(words)) )
}
alpha = .1
n_topics = 15
lda_results <- FitLdaModel(dtm = dtm_train,
k = n_topics, # number of topics
burnin = 200 + 10*n_topics,
iterations = 700 + 10*n_topics,
alpha = alpha, beta = 0.05,
optimize_alpha = T,
calc_likelihood = T,
calc_coherence = T)
model <- lda_results
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts.
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
plot(model$prevalence, model$alpha, xlab = "% topic prevalence", ylab = "alpha")
# Get the top M terms of each topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
model$top_terms
SummarizeTopics(lda_results)
# Get top 10 words with probabilities for visualization
phi <- lda_results$phi
top_term_table <- NULL
n_topics = 15
for (j in 1:n_topics) {
words <- t(phi[j,]) %>% order(decreasing=TRUE) %>% head(10) %>% phi[j,.]
top_term_table <- rbind(top_term_table, data.frame(topic=j,probability=words , term = labels(words)) )
}
# Create a table of top 10 words grouped by topic
text_top_terms <- top_term_table %>%
group_by(topic) %>%
top_n(10, probability) %>%
ungroup() %>%
arrange(topic, -probability)
# Create plots of top 10 words for each topic
perplot <- 6
for (i in 1:ceiling(n_topics/perplot))
{
p <- text_top_terms %>%
filter(topic > (i-1)*perplot & topic<=(i*perplot)) %>% # filter out topic
mutate(term = reorder_within(term, probability, topic)) %>% # show highest probability up top
ggplot(aes(term, probability, fill = factor(topic))) + # bar plot layout
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()+
scale_x_reordered()
show(p)
}
# Get top 10 words with probabilities for visualization
phi <- lda_results$phi
top_term_table <- NULL
n_topics = 15
for (j in 1:n_topics) {
words <- t(phi[j,]) %>% order(decreasing=TRUE) %>% head(10) %>% phi[j,.]
top_term_table <- rbind(top_term_table, data.frame(topic=j,probability=words , term = labels(words)) )
}
# Create a table of top 10 words grouped by topic
text_top_terms <- top_term_table %>%
group_by(topic) %>%
top_n(10, probability) %>%
ungroup() %>%
arrange(topic, -probability)
# Create plots of top 10 words for each topic
perplot <- 6
for (i in 1:ceiling(n_topics/perplot))
{
p <- text_top_terms %>%
filter(topic > (i-1)*perplot & topic<=(i*perplot)) %>% # filter out topic
mutate(term = reorder_within(term, probability, topic)) %>% # show highest probability up top
ggplot(aes(term, probability, fill = factor(topic))) + # bar plot layout
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()+
scale_x_reordered()
show(p)
}
library(rvest)
page <- read_html(url)
url <- "https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+9+7950X&id=5031"
page <- read_html(url)
DDR5_names <- page %>%
html_elements("//*[@id="placeholder"]/div/div[2]/div") %>%
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
DDR5_names
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
View(df)
write.csv(CPU_complete, file = "../../final_data/CPU.csv")
source("D:/Erasmus University/thesis_PC_LTE/data_scraping/R_code/Merge_Data.R", echo=TRUE)
setwd("D:/Erasmus University/thesis_PC_LTE/data_scraping/R_code")
source("D:/Erasmus University/thesis_PC_LTE/data_scraping/R_code/Merge_Data.R", echo=TRUE)
write.csv(CPU_complete, file = "../../final_data/CPU.csv")
setwd("D:/Erasmus University/thesis_PC_LTE/data_scraping/R_code")
write.csv(CPU_complete, file = "../../final_data/CPU.csv")
write.csv(CPU_complete, file = "../../CPU.csv")
write.csv(CPU_complete, file = "../../final_data/CPU.csv")
source("D:/Erasmus University/thesis_PC_LTE/data_scraping/R_code/Merge_Data.R", echo=TRUE)
write.csv(CPU_complete, file = "../../final_data/CPU.csv")
write.csv(GPU_complete, file = "../../final_data/GPU.csv")
write.csv(Disk_complete, file = "../../final_data/Disk.csv")
write.csv(Memory_complete, file = "../../final_data/Memory.csv")
